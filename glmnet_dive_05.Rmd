---
title: "glmnetをもう少し理解したい⑤"
output: html_document
---

それでは前回の記事に続いて`elnet1`の紹介です。前回の記事はこちらです。

前回の記事のリンク

[:contents]

#### ループ③
以上までで見てきた通り、ループ①・②では `alm`すなわち`lambda`を更新しつつ、`alpha`（`alf`）や`penalty.factor`（`vp`）との乗算によって罰則を計算していました。
ループ③ではその罰則を用いて回帰係数を更新します。
なのでこのループが`glmnet`においてメインとなる処理と言って良いと思います。

ループ③は`ni`に対するループです。ここで`ni`は説明変数の数ですね。k をインデックスとして各説明変数をさらっていきます。

まず`ju`ですが、これは各説明変数列における数値のバラつきの有無を示す 1/0 のベクトルでした。バラつきがない、すなわち全ての数値が同じであれば（`ju(k) == 0` ）ループ③をスキップします（`goto`の向かう先が 10371 で、ループの範囲も同じく 10371 となっています）。

```{fortran}
      do 10371 k=1,ni
      if(ju(k).eq.0) goto 10371
```

次に`a`から k 番目の変数の値を`ak`に格納します。前回記事で追いかけた通り、この`a`（または`ao`）が最終的には回帰係数として返ります。

前処理において`a = 0.0`で初期化されているのでループの 1 周目時点では`ak`も 0 ですが、ループ①の 2 周目以降は縮小された回帰係数が入っています。

```{fortran}
      ak=a(k) ! k 番目の変数の a の値を ak に代入。
```

続いて`u`と`v`を計算します。これらは前回の記事で少し紹介した通り、次のブロックで回帰係数`a`を更新するためのものです。

`u`は`g(k)`に`ak*xv(k)`を加算して計算します。ここで`g(k)`は`standerd`において`g(j)=dot_product(y,x(:,j)) `、つまり`y`と`x`の内積として定義されたものでした（`y`と`x`はそれぞれ標準化されています）。もしも罰則が付いていなければこの共分散が OLS による回帰係数となるはずです（標準化されているので`x`の標準偏差は 1）。

この`g`に`xv`で重みをつけた`ak`を加算します。ここで`xv`は weight を乗じた`x`の二乗和です。しかしループの 1 周目では`ak=0`であるため`g`がそのまま利用されることになります。

このようにして定義された`u`の絶対値から罰則を減じたものが`v`となります。

```{fortran}
      u=g(k)+ak*xv(k)
      v=abs(u)-vp(k)*ab
```

そしてさらに`v`が 0 よりも大きい場合（OLS による回帰係数が罰則よりも大きい場合）、
 - 「`cl(2,k)`」と「`sign(v,u)/(xv(k)+vp(k)*dem)`」を比較して小さい方を選ぶ
 - それを「`cl(1,k)`」と比較して大きい方を選ぶ
という処理を行い、新たに`a`として格納します。
ここで`cl`は`glmnet.r`で`cl = rbind(lower.limits, upper.limits)` として定義されたものなので、推定された値を上限と下限の間に抑えようとしていることがわかります。また`v`が 0 以下の場合は 0 となります。

```{fortran}
      ! a(k) を更新
      a(k)=0.0
      if(v.gt.0.0) a(k)=max(cl(1,k),min(cl(2,k),sign(v,u)/(xv(k)+vp(k)*dem)))
```

以上が回帰係数の更新を行う処理になります。
ややアッサリしていますが、ここの処理は glmnet を理解する上で極めて重要なのでもう少し説明します。

まず前提として、（Elastic Net ではなく）Lasso では軟閾値作用素と呼ばれる写像を用いて解を推定しています。
ここで軟閾値作用素とは、定数 $a$ および $λ (> 0)$ において $a$ の絶対値が $λ$よりも大きければ $a-λ$ を、そうでなければ 0 を返す作用素です：

$ S(a, \lambda) = \begin{cases} a - \lambda & (a > \lambda) \\ 0 & (|a| \le \lambda) \\ a + \lambda & (a < -\lambda) \end{cases} $


すなわち、<bold>推定された回帰係数（の絶対値）が罰則よりも小さければ 0 に丸めてしまい、大きくても罰則の分だけ係数を縮小してしまう</bold>、ということです。
一般に Lasso は効果の小さな変数の回帰係数を 0 に縮小する方法として知られていますが、実装としてはこのような軟閾値作用素が用いられており、これを見ると「Lasso はスパースな解を推定できる」という言葉の意味がわかるのではないでしょうか。
推定したら 0 になるわけではなく、明示的に 0 にしているのだと。

ここで少し余談なのですが、Lasso や Ridge に関する参考書などを読んでいると「幾何学的な説明」として以下のようなグラフが描かれることがよくあると思います：

例のグラフ

このグラフを見るたびに私は納得いかない気分になっていました。と言うのも、Lasso の方（グラフ左側）に着目すると、OLS による推定値の座標（グラフ中の×印の位置）や楕円の広がり方によっては菱形の頂点ではなく辺に接することが普通にあり得そうだからです。
少なくともこのグラフをもって「Lasso は菱形の頂点に接しやすい（ゆえに解が 0 と推定されやすい）」というのは全く自明ではないし直感的でもないな、と思っていました。

そんな時に「機械学習の数理100問シリーズ」の「スパース推定100問 with R」を読んでいると、またも上記のようなグラフが出てきたので悶々としたのですが、次のページには以下のようなグラフがありました：

あのグラフ

まさにこれです。このグラフにおいて白色の部分に OLS の推定値がある場合、頂点ではなく辺に接することになります。そこから少しずれて緑色の部分に OLS の推定値が存在する場合には菱形の頂点に接することとなる、つまりいずれか重要でない方の解が 0 として推定されるようになります。

上のグラフのような「幾何学的な説明」は本当に多くの本・記事で見かけるのですが、下のグラフも合わせて説明することでより理解が深まるのでは、と思いました。
余談おわり。

さて、上記のブロックでは、回帰係数が罰則よりも大きく、かつ上限・下限の範囲内であれば`sign(v,u)/(xv(k)+vp(k)*dem)`を新たな`a`とするのでした。
さきほどの軟閾値作用素の説明においては「罰則を減じた回帰係数」（つまり`v`）をLasso推定値としていましたが、ここではそれを`xv(k)+vp(k)*dem`で除しています。
これは、ここで得ようとしている推定値というのが Lasso ではなく Elastic Net であるためであり、（第一回で紹介した）教科書（P36）では Elastic Net の推定量を

$ \hat{\beta}^{EN}_{j} = \begin{cases} (\hat{\beta}^{OLS}_{j} - \lambda_{1})/(1+\lambda_{2}) & (\hat{\beta}^{OLS}_{j} > \lambda_{1}) \\ 0 & (|\hat{\beta}^{OLS}_{j}| \le \lambda_{1}) \\ (\hat{\beta}^{OLS}_{j} + \lambda_{1})/(1+\lambda_{2}) & (\hat{\beta}^{OLS}_{j} < -\lambda_{1}) \end{cases} $

としています。`dem`は`alm*(1-bta)`で定義されていたことを思い出すと、これは Ridge （L2）に対する罰則であり、上記の式では$\lambda_{2}$に該当します。
また`xv`は X の二乗和を分散で除して 1 を加算したものですが、これが何を意味しているのかは以前紹介したときもわからなかったのですが、サンプルデータを使って計算してみるとおおよそ 1 になりそうなのできっとそういう数値なんだろうと思います（適当）。


残る処理ですが、上記によって`a(k)`が更新されなければループを抜けて次の変数に移ります（`goto`の移動先`10371`はループ③の終点でした）。

! mm は lambda ループの１回目では 0 なので１回目だけ処理を行う？
! 10391 は４番目のループの先なので、 mm が 0 でなければ ４番目のループをスキップ
! mm(k) が 0 なら nin を +1 する。 おそらく、パラメータが 0 でないときに mm は 0 となる。    
! nx は非ゼロとする変数の上限なので、推定したパラメータ数がそれを越えると３番目のループを抜ける

```{fortran}
      if(a(k).eq.ak) goto 10371
      if(mm(k) .ne. 0) goto 10391 
      nin=nin+1                                                    
      if(nin.gt.nx)goto 10372 
```







##### ループ④

```{fortran}
      ! ４番目のループ
      ! 分散共分散行列のようなものを作っている
      ! ここでもループの対象は説明変数（ただしインデックスは k ではなく j）
      do 10401 j=1,ni
      ! バラツキがなければ以降の処理をスキップ

      if(ju(j).eq.0)goto 10401
      ! mm が 0（パラメータが 0 でない）なら 以降をスキップ。
      if(mm(j) .eq. 0)goto 10421
      c(j,nin)=c(k,mm(j))
      goto 10401
10421 continue
      if(j .ne. k)goto 10441  ! 変数が同一でなければ 10441 に飛ぶ
      c(j,nin)=xv(j) ! 同一だったらここ
      goto 10401
10441 continue
      c(j,nin)=dot_product(x(:,j),x(:,k)) ! 同一でなかったら j と k の内積をとる
10401 continue ! ４番目のループはここまで

      continue
      ! mm に nin を入れる
      mm(k)=nin
      ! ia に k を格納
      ia(nin)=k ! 0 でないパラメータが推定された変数の位置
10391 continue   
      ! a(k) の差分をとる。 a(k)、 ak は推定された回帰係数。
      del=a(k)-ak
      ! 残差平方和を更新する
      rsq=rsq+del*(2.0*g(k)-del*xv(k))
      ! rsq = rsq + del * (2.0 * g(k) - del * xv(k))
      ! rsq は残差平方和
      ! del は a(k)-ak
      ! g(k) は縮小前の回帰係数（y と x(k) の内積）、そこから weight 調整済みの x の二乗和 を減じる
      dlx=max(xv(k)*del**2,dlx)
```

##### ループ⑤



```{fortran}
      ! ５番目のループ
      ! 探索範囲は三度説明変数
      do 10451 j=1,ni ! インデックスは再度 j を使う
      if(ju(j).ne.0) g(j)=g(j)-c(j,mm(k))*del                           
10451 continue ! ５番目のループはここまで
      continue
10371 continue ! ３番目のループはここまで

10372 continue
      if(dlx.lt.thr)goto 10352
      if(nin.gt.nx)goto 10352
      if(nlp .le. maxit)goto 10471
      jerr=-m
      return
10471 continue
10360 continue
      iz=1
      da(1:nin)=a(ia(1:nin))
      continue
10481 continue
      nlp=nlp+1
      dlx=0.0
```



#### ループ⑥

```{fortran}
      ! ６番目のループ
      ! ３番目のループと同じことを ni ではなく nin に対して再度実行
      do 10491 l=1,nin
      k=ia(l) ! k を取り出す（ ia には 0 ではないパラメータが推定された変数の列が格納されてる）
      ak=a(k) ! a を取り出す
      u=g(k)+ak*xv(k)
      v=abs(u)-vp(k)*ab
      a(k)=0.0
      if(v.gt.0.0) a(k)=max(cl(1,k),min(cl(2,k),sign(v,u)/(xv(k)+vp(k)*dem)))
      if(a(k).eq.ak)goto 10491
      del=a(k)-ak
      rsq=rsq+del*(2.0*g(k)-del*xv(k))
      dlx=max(xv(k)*del**2,dlx)
```


##### ループ⑦

```{fortran}
      ! ７番目のループ
      ! 上と同様、 nin に対して g を更新
      do 10501 j=1,nin
      g(ia(j))=g(ia(j))-c(ia(j),mm(k))*del
10501 continue ! ７番目のループここまで
      continue
10491 continue ! ６番目のループここまで

      continue
      if(dlx.lt.thr)goto 10482
      if(nlp .le. maxit)goto 10521
      jerr=-m
      return
10521 continue
      goto 10481  ! えっ！！！
10482 continue
      da(1:nin)=a(ia(1:nin))-da(1:nin)
```


#### ループ⑧

```{fortran}
      ! ８番目のループ
      do 10531 j=1,ni
      if(mm(j).ne.0)goto 10531
      if(ju(j).ne.0) g(j)=g(j)-dot_product(da(1:nin),c(j,1:nin))
10531 continue ! ８番目のループここまで
                                                         
      continue
      jz=0
      goto 10351  ! えっ！！ ３番目のループの開始まで戻すの！
10352 continue
      if(nin .le. nx)goto 10551  ! nin が nx を超えた場合はここにくる
      jerr=-10000-m
      goto 10282 ! jerr を 更新して elnet1 を抜ける
10551 continue
      if(nin.gt.0) ao(1:nin,m)=a(ia(1:nin))
      kin(m)=nin   ! m 回目のループの nin を kin[m] に格納する
      rsqo(m)=rsq  ! m 回目のループの rsq を rsqo[m] に格納する
      almo(m)=alm  ! m 回目のループの alm を almo[m] に格納する
      lmu=m
      if(m.lt.mnl)goto 10281
      if(flmin.ge.1.0)goto 10281
      me=0
```


#### ループ⑨

```{fortran}
      ! ９番目のループ
      do 10561 j=1,nin
      if(ao(j,m).ne.0.0) me=me+1
10561 continue ! ９番目のループここまで 

      continue
      if(me.gt.ne)goto 10282
      if(rsq-rsq0.lt.sml*rsq)goto 10282
      if(rsq.gt.rsqmax)goto 10282
10281 continue ! lambda のループはここまで

10282 continue
      deallocate(a,mm,c,da)
      return
      end
```


