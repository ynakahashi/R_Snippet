---
title: "Gaussian Process 03"
author: "ynakahashi"
date: "2019/3/18"
output: html_document
---

## 「ガウス過程と機械学習」第3章のグラフ（一部）を作図する③

前回の記事の続きです。


今回は図3.16と3.20に挑戦します。データはサポートサイト(http://chasen.org/~daiti-m/gpbook/data/gpr.dat)から取得しました。

```{r}
dat <- read.table("http://chasen.org/~daiti-m/gpbook/data/gpr.dat", sep = "\t",
                  col.names = c("x", "y"))
train <- head(dat, 5)
```

まずは図3.16から。取得したデータをプロットしてみます。

```{r}
plot(train, ylim = c(-1, 3), xlab = "x", pch = 4, cex = 2)
```

本ではX軸の数値が書かれていませんが、多分これで合っています。

このデータからRBFカーネルを使って`x`の共分散行列を作成しますが、前回定義したRBFカーネルは入力としてスカラを想定していました。しかし`x`はベクトルとなることもあるため、RBFカーネルを少し修正します。

```{r}
## 前回定義したRBFカーネル
rbf_knl_old <- function(x1, x2, theta1 = 1, theta2 = 1) {
   t(apply(as.matrix(x1), 1, function(x) theta1 * exp(-(x-x2)^2/theta2)))
}

## ベクトルに対応するために修正
rbf_knl <- function(x1, x2, theta1 = 1, theta2 = 1) {
   theta1 * exp(-norm(x1 - x2, "2")^2/theta2)
}
```

ベクトル同士の距離を計算するために`norm`関数を使いました。ここでは距離としてL2ノルム（ユークリッド距離）を使用していますが、その後に2乗していることに注意してください。

上でベクトルでの入力に対応した代わりに共分散行列を一度に求めることができなくなってしまったので、`get_cov_mat`も以下のように定義し直します：

```{r}
## d次元のベクトルを要素とするx1[n_1, d]およびx2[n_2, d]について、x1_nとx2_n'の共分散を計算する
get_cov_mat <- function(x1, x2, theta1 = 1, theta2 = 1) {
   ## matrixに変換
   x1 <- as.matrix(x1)
   x2 <- as.matrix(x2)
   
   ## 行の組み合わせを作成する
   n <- nrow(x1)
   m <- nrow(x2)
   d <- ncol(x1)
   tmp <- cbind(kronecker(x1, matrix(1, m)), kronecker(matrix(1, n), x2))
   
   ret <- apply(tmp, 1, function(x, d, theta1, theta2) {
      rbf_knl(x[(1:d)], x[(d+1):ncol(tmp)], theta1, theta2)
   }, d, theta1, theta2)
   return(matrix(ret, n, m, byrow = T))
}
```

ここでは

 1. `kronecker`関数を使い、元の行列`x1`と`x2`について全ての行の組み合わせを持つ`(n*m, 2*d)`のサイズとなる行列を新たに作成し（`tmp`）
 1. 各行について、前半`d`個の要素を持つベクトルと後半`d`個のベクトルを抽出してカーネルを計算する（`ret`）

という少しややこしい手順を踏んでいます。ここは本来なら

```{r}
x1 <- matrix(seq(0, 1, length.out = 15), 5, 3, byrow = T)
x2 <- matrix(seq(1, 2, length.out = 12), 4, 3)
apply(x1, 1, rbf_knl, x2[1, ]) # x2の1行目だけでなく、各行を順に渡したい！
```

このような形で`apply`を使ってすっきり書きたいのですが、`apply`を二重で渡す方法がわからなかったため、一度すべての組み合わせを作成することにしました。

ひとまずこれでサンプルデータの共分散行列を見てみましょう。

```{r}
get_cov_mat(train$x, train$x)
```


また、`for`を使って素直に書いた場合の結果と一致するかも確認しておきましょう：

```{r}
n <- nrow(train)
K <- matrix(0, n, n)
for (i in 1:n) {
   for (j in 1:n) {
      if (i > j) next
      K[i, j] <- K[j, i] <- rbf_knl(train$x[i], train$x[j])
   }
}
K
```

合っているようです。

共分散行列が数字のままだと結果がわかりにくいのでヒートマップを作成してみます。

```{r}
tmp <- get_cov_mat(train$x, train$x)
heatmap(tmp, Rowv = NA, Colv = NA, revC = T)
```

おや、なんか色が変ですね、対角要素は同じ色になるはずなのですが。`gplots`の`heatmap.2`関数を使ってみます。

```{r}
library(gplots)
heatmap.2(get_cov_mat(train$x, train$x), Rowv = NA, Colv = NA, dendrogram = "none", 
          trace = "none")
```

こっちは良さそうですね。

```{r}
K <- get_cov_mat(train$x, train$x)

n <- 1
mu <- rep(0, nrow(train))
```

```{r}
set.seed(123)
y <- MASS::mvrnorm(n = n, mu = mu, Sigma = K)
plot(y, type = "l")
```

良さそうです。

ところでこのグラフを何回かプロットしてみるとわかりますが、`y[1]`と`y[2]`および`y[4]`と`y[5]`の間には、他と比べて大きな差が生じやすい傾向にあります。ヒートマップを見るとわかりやすいのですが、これらは共分散が小さい（"似ていない"）ため、異なる値となりやすいんですね。そのため`y`のプロットも少し滑らかさに欠けるものとなっています。


図3.17のアルゴリズムを参考に、`y`の平均と分散を推定します。なお`theta1`、`theta2`および`theta3`はそれぞれ`1`、`0.4`、`0.1`とのことですが、作図の印象を図3.16に近づけるために`theta3`を`0.04`としました。

```{r}
test <- seq(-1, 3.5, 0.05)
theta1 <- 1
theta2 <- 0.4
theta3 <- 0.04 # 本では0.1とのこと
M <- length(test)
N <- nrow(train)

## x * xの共分散行列を計算する
K <- get_cov_mat(train$x, train$x, theta1 = theta1, theta2 = theta2)

## 対角要素に誤差分散を加える
diag(K) <- diag(K) + theta3

## x * new_xの共分散行列を計算する
k   <- get_cov_mat(train$x, test, theta1 = theta1, theta2 = theta2)

## new_x * new_xの共分散行列を計算する
s   <- get_cov_mat(test, test, theta1 = theta1, theta2 = theta2)
diag(s)   <- diag(s) + theta3

## 平均の推定
Mu  <- t(k) %*% solve(K) %*% train$y # yyはここでまとめて計算した

## 分散の推定
Var <- s - t(k) %*% solve(K) %*% k

## 信頼区間
CI_u  <- Mu + 2 * sqrt(diag(Var))
CI_l  <- Mu - 2 * sqrt(diag(Var))
```


```{r}
plot(train, xlim = c(-0.5, 3.5), ylim = c(-1, 3), type = "n")
polygon(c(rev(test), test), c(CI_u, CI_l), col = 'gray', border = NA)
points(train$x, train$y, xlim = c(-0.5, 3.5), ylim = c(-1, 3), xlab = "x", pch = 4, cex = 2)
lines(test, Mu, xlim = c(-0.5, 3.5), ylim = c(-1, 3), type = "l", ylab = "")
```


ちょっと本のグラフとは異なりますが、同様のプロットを作成することが出来ました。

本にも記載のあることですが、上記の計算では一般の機械学習アルゴリズムで認められる「反復的な処理によって点推定値を求める工程」（いわゆるフィッティングや最適化）は一切行われていません。平均も分散も、行列の計算によって解析的に得られています。**ガウス過程回帰においては学習は存在しない**ということは大事なポイントだと思います。





